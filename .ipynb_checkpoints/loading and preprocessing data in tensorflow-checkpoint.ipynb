{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will practice how to load and preprocess data using tensorflow's data API. \n",
    "\n",
    "Also, see the following official [guide](https://www.tensorflow.org/guide/data#basic_mechanics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1\n",
    "\n",
    "This is excercise 13.9 in [this](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) book: \n",
    "\n",
    "### Problem statement:\n",
    "\n",
    "Load the Fashion MNIST dataset; split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label. Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:2.1.0\n",
      "keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('tensorflow version:{}'.format(tf.__version__))\n",
    "print('keras version: {}'.format(keras.__version__))\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fashion-MNIST dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist \n",
    "# the above is the fashion_mnist module\n",
    "# call fashion_mnist.load_data() to download the data using the above module\n",
    "# fashion_mnist.load_data returns one tuple of training images and their labels, and another tuple of test images and their labels\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images in training set: 60000\n",
      "number of images in test set: 10000\n"
     ]
    }
   ],
   "source": [
    "print('number of images in training set: {}'.format(len(X_train_full)))\n",
    "print('number of images in test set: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_full is of type: <class 'numpy.ndarray'>\n",
      "y_train_full is of type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# the images are stored as numpy arrays\n",
    "print('X_train_full is of type: {}'.format(type(X_train_full)))\n",
    "print('y_train_full is of type: {}'.format(type(y_train_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_full:(60000, 28, 28)\n",
      "minimum pixel value: 0\n",
      "maximum pixel value: 255\n"
     ]
    }
   ],
   "source": [
    "print('shape of X_train_full:{}'.format(X_train_full.shape))\n",
    "print('minimum pixel value: {}'.format(X_train_full.min()))\n",
    "print('maximum pixel value: {}'.format(X_train_full.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X_train_full into a training set and a validation set\n",
    "num_val = 10000 # number of instances in validation set\n",
    "X_val, X_train = X_train_full[:num_val], X_train_full[num_val:]\n",
    "y_val, y_train = y_train_full[:num_val], y_train_full[num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \n",
    "               \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 90\n",
    "plt.figure()\n",
    "plt.imshow(X_train[idx]/255.0)\n",
    "plt.title(class_names[y_train[idx]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specs of train_imgs: (TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n",
      "specs of val_imgs: (TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n",
      "specs of test_imgs: (TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n"
     ]
    }
   ],
   "source": [
    "# create tensorflow datasets from training, validation and test images\n",
    "train_imgs = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_imgs = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_imgs = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "print('specs of train_imgs: {}'.format(train_imgs.element_spec))\n",
    "print('specs of val_imgs: {}'.format(val_imgs.element_spec))\n",
    "print('specs of test_imgs: {}'.format(test_imgs.element_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training set\n",
    "seed = 42\n",
    "buffer_size = 5000\n",
    "train_shuffle = train_imgs.shuffle(buffer_size = buffer_size, seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert an individual record from the dataset into a serialized Example protobuf\n",
    "def to_Example_protobuf(record):\n",
    "    '''Convert a record to a serialized Example protobuf. The record is assumed to be a tuple with the first entry being a tensor and the second entry being its label\n",
    "    '''\n",
    "    \n",
    "    # convert the first entry of the record i.e. the image, to a Feature class\n",
    "    # tensors should be converted to binary strings first\n",
    "    bin_string = tf.io.serialize_tensor(record[0]).numpy()\n",
    "    # since it is a string we can only store it as a BytesList but NOT as a FloatList or Int64List\n",
    "    bytes_list = tf.train.BytesList(value = [bin_string]) # note that the value in BytesList has to be a list\n",
    "    # convert the above BytesList to a Feature\n",
    "    img = tf.train.Feature(bytes_list = bytes_list)\n",
    "    \n",
    "    # convert the second entry in the record i.e. the image-label to a Feature class\n",
    "    # note that label are integers, so can be stored as Int64List\n",
    "    int64_list = tf.train.Int64List(value = [record[1].numpy()])\n",
    "    lbl = tf.train.Feature(int64_list = int64_list)\n",
    "    \n",
    "    # now add img and lbl, as obtained above, to a dictionary \n",
    "    # then use it to create an instance of the Features class\n",
    "    feature = {'img': img, 'lbl': lbl}\n",
    "    features = tf.train.Features(feature = feature)\n",
    "    \n",
    "    # use the features object above, to create an Example\n",
    "    example = tf.train.Example(features = features)\n",
    "    \n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a serialized Example protobuf object created by to_Example_protobuf\n",
    "# and convert it to a tuple of tensor corresponding to an image and its label\n",
    "def from_Example_protobuf(example):\n",
    "    \n",
    "    out_type = tf.uint8 # the img tensor had type tf.unint8 before we converted it to Example using to_Example_protobuf\n",
    "    img = tf.io.parse_tensor(example.features.feature['img'].bytes_list.value[0], out_type = out_type)\n",
    "    \n",
    "    lbl = tf.convert_to_tensor(example.features.feature['lbl'].int64_list.value[0], dtype = tf.uint8)\n",
    "    return img, lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test to_Example_protobuf and from_Example_protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a record from train_shuffle\n",
    "itr = iter(train_shuffle)\n",
    "item = next(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  feature {\n",
       "    key: \"img\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\0006\\336\\263\\221\\216\\307\\344 \\000\\000\\000\\001\\001\\000\\002\\000\\000\\000\\000\\000\\001\\001\\000\\001\\000\\000`\\307\\373\\320\\316\\334\\320\\340\\302\\344\\302U\\000\\000\\001\\002\\001\\000\\000\\000\\000\\000\\000\\001\\000\\000M\\303\\334\\321\\310\\315\\314}\\213\\334\\264\\275\\306\\315\\264A\\000\\000\\000\\000\\000\\000\\000\\000\\000\\002\\000H\\335\\314\\306\\307\\304\\310\\312\\304\\327\\276\\267\\307\\275\\271\\311\\3117\\000\\000\\000\\000\\000\\000\\000\\001\\000\\010\\314\\327\\307\\311\\303\\303\\304\\310\\310\\301\\304\\300\\300\\300\\300\\271\\304\\275\\t\\000\\000\\000\\000\\000\\000\\000\\000\\266\\323\\331\\301\\307\\300\\303\\276\\303\\274\\250\\303\\302\\301\\275\\275\\271\\267\\321\\241\\000\\000\\000\\000\\000\\000\\000I\\331\\276\\341\\275\\314\\300\\300\\302\\312\\303\\271\\302\\275\\274\\276\\274\\271\\271\\302\\317/\\000\\000\\000\\000\\000\\000\\327\\310\\307\\363\\273\\276\\343\\342\\340\\336\\302\\307\\300\\275\\272\\275\\273\\264\\303\\301\\311\\302\\000\\000\\000\\000\\000q\\327\\274\\314\\366\\267\\310\\302\\275\\267\\272\\300\\302\\276\\273\\275\\272\\276\\276\\276\\303\\267\\320n\\000\\000\\000\\000;\\276\\314\\304\\342\\274\\310\\302\\311\\307\\307\\271\\274\\266\\261\\273\\266\\267\\273\\272\\274\\311\\273K\\000\\000\\000\\000\\000\\003\\300\\341\\340\\267\\306\\323\\306\\320\\325\\311\\321\\312\\316\\307\\306\\307\\275\\267\\316\\255\\002\\000\\000\\000\\000\\000\\001\\000\\000\\202\\377\\300\\301\\275\\272\\303\\265\\263\\273\\273\\267\\301\\302\\300\\275\\300g\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000}\\316\\274\\276\\273\\276\\273\\264\\275\\274\\265\\271\\267\\257\\327i\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\002\\000\\206\\317\\271\\272\\274\\301\\267\\265\\275\\274\\264\\273\\273\\257\\304i\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\212\\317\\271\\300\\272\\300\\264\\266\\275\\273\\261\\274\\271\\261\\304o\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\213\\317\\273\\300\\271\\304\\260\\264\\275\\272\\256\\274\\272\\260\\303t\\000\\002\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\217\\316\\272\\300\\273\\307\\257\\265\\300\\273\\256\\272\\274\\260\\301s\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\224\\315\\272\\276\\276\\306\\257\\272\\276\\300\\263\\271\\274\\261\\301s\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\227\\315\\272\\275\\301\\306\\257\\273\\273\\300\\260\\266\\274\\261\\300u\\000\\002\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\240\\314\\274\\276\\302\\307\\256\\274\\272\\302\\266\\266\\275\\267\\302t\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\243\\315\\275\\300\\302\\302\\261\\301\\272\\304\\272\\267\\275\\271\\303t\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\244\\320\\276\\302\\306\\276\\265\\307\\274\\306\\272\\271\\300\\271\\303z\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\246\\323\\300\\303\\315\\301\\256\\314\\303\\314\\276\\273\\301\\272\\303~\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\244\\327\\302\\303\\315\\301\\260\\303\\306\\306\\301\\273\\300\\273\\304\\202\\000\\002\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\241\\327\\302\\306\\317\\301\\265\\303\\310\\310\\306\\275\\276\\273\\303|\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\235\\325\\302\\306\\310\\267\\257\\273\\276\\300\\275\\271\\274\\271\\302o\\000\\005\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\267\\346\\320\\364\\373\\351\\343\\352\\355\\360\\356\\351\\347\\310\\324z\\000\\005\\000\\000\\000\\000\\000\\000\\000\\000\\002\\0008jMSUQQSQSTQNKc\\036\\000\\003\\000\\000\\000\\000\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"lbl\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 0\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the record to Example \n",
    "ex = to_Example_protobuf(item)\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image data was converted to Example and converted back faithfully: True\n",
      "label data was converted to Example and converted back faithfully: True\n"
     ]
    }
   ],
   "source": [
    "# re-obtain the record from the Example protobuf and check its validity\n",
    "img, lbl = from_Example_protobuf(ex)\n",
    "print('image data was converted to Example and converted back faithfully: {}'.format(tf.math.reduce_all(img == item[0])))\n",
    "print('label data was converted to Example and converted back faithfully: {}'.format(tf.math.reduce_all(lbl == item[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
