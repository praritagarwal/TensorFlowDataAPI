{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will practice how to load and preprocess data using tensorflow's data API. \n",
    "\n",
    "Also, see the following official [guide](https://www.tensorflow.org/guide/data#basic_mechanics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1\n",
    "\n",
    "This is excercise 13.9 in [this](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) book: \n",
    "\n",
    "### Problem statement:\n",
    "\n",
    "Load the Fashion MNIST dataset; split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label. Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:2.1.0\n",
      "keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('tensorflow version:{}'.format(tf.__version__))\n",
    "print('keras version: {}'.format(keras.__version__))\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fashion-MNIST dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist \n",
    "# the above is the fashion_mnist module\n",
    "# call fashion_mnist.load_data() to download the data using the above module\n",
    "# fashion_mnist.load_data returns one tuple of training images and their labels, and another tuple of test images and their labels\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images in training set: 60000\n",
      "number of images in test set: 10000\n"
     ]
    }
   ],
   "source": [
    "print('number of images in training set: {}'.format(len(X_train_full)))\n",
    "print('number of images in test set: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_full is of type: <class 'numpy.ndarray'>\n",
      "y_train_full is of type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# the images are stored as numpy arrays\n",
    "print('X_train_full is of type: {}'.format(type(X_train_full)))\n",
    "print('y_train_full is of type: {}'.format(type(y_train_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_full:(60000, 28, 28)\n",
      "minimum pixel value: 0\n",
      "maximum pixel value: 255\n"
     ]
    }
   ],
   "source": [
    "print('shape of X_train_full:{}'.format(X_train_full.shape))\n",
    "print('minimum pixel value: {}'.format(X_train_full.min()))\n",
    "print('maximum pixel value: {}'.format(X_train_full.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X_train_full into a training set and a validation set\n",
    "num_val = 10000 # number of instances in validation set\n",
    "X_val, X_train = X_train_full[:num_val], X_train_full[num_val:]\n",
    "y_val, y_train = y_train_full[:num_val], y_train_full[num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \n",
    "               \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 90\n",
    "plt.figure()\n",
    "plt.imshow(X_train[idx]/255.0)\n",
    "plt.title(class_names[y_train[idx]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specs of train_imgs: (TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n",
      "specs of val_imgs: (TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n",
      "specs of test_imgs: (TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n"
     ]
    }
   ],
   "source": [
    "# create tensorflow datasets from training, validation and test images\n",
    "train_imgs = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_imgs = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_imgs = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "print('specs of train_imgs: {}'.format(train_imgs.element_spec))\n",
    "print('specs of val_imgs: {}'.format(val_imgs.element_spec))\n",
    "print('specs of test_imgs: {}'.format(test_imgs.element_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert an individual record from the dataset into a serialized Example protobuf\n",
    "def to_Example_protobuf(record):\n",
    "    '''Convert a record to a serialized Example protobuf. The record is assumed to be a tuple with the first entry being a tensor and the second entry being its label\n",
    "    '''\n",
    "    \n",
    "    # convert the first entry of the record i.e. the image, to a Feature class\n",
    "    # tensors should be converted to binary strings first\n",
    "    bin_string = tf.io.serialize_tensor(record[0]).numpy()\n",
    "    # since it is a string we can only store it as a BytesList but NOT as a FloatList or Int64List\n",
    "    bytes_list = tf.train.BytesList(value = [bin_string]) # note that the value in BytesList has to be a list\n",
    "    # convert the above BytesList to a Feature\n",
    "    img = tf.train.Feature(bytes_list = bytes_list)\n",
    "    \n",
    "    # convert the second entry in the record i.e. the image-label to a Feature class\n",
    "    # note that label are integers, so can be stored as Int64List\n",
    "    int64_list = tf.train.Int64List(value = [record[1].numpy()])\n",
    "    lbl = tf.train.Feature(int64_list = int64_list)\n",
    "    \n",
    "    # now add img and lbl, as obtained above, to a dictionary \n",
    "    # then use it to create an instance of the Features class\n",
    "    feature = {'img': img, 'lbl': lbl}\n",
    "    features = tf.train.Features(feature = feature)\n",
    "    \n",
    "    # use the features object above, to create an Example\n",
    "    example = tf.train.Example(features = features)\n",
    "    \n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse an Example protobuf or a serialized string obtained from it\n",
    "# if batch == True, then the input is assumed to be a batch rather than a single example \n",
    "def from_Example_protobuf(example, batch = False):\n",
    "    ''' function to parse a example protobuf generated from examples in fashion-MNIST dataset. The input can be an Example protobuf or a serialized string obtained from an Example protobuf The input to this function must be a serialized string '''\n",
    "    feature_description = {\n",
    "        'img': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'lbl': tf.io.FixedLenFeature([], tf.int64, default_value = 0)\n",
    "        }\n",
    "    \n",
    "    if batch:\n",
    "        parsed_examples = tf.io.parse_example(example, feature_description)\n",
    "        # parsed_examples['img'] is itelf a tensor whose entries are serialized strings corresponding to the images in the batch\n",
    "        # we therefore need to parse these strings back to tensors using tf.io.parse_tensor\n",
    "        # This can be applied to the elements of parsed_examples['imgs'] using tf.map_fn\n",
    "        imgs = tf.map_fn(lambda string: tf.io.parse_tensor(string, out_type = tf.uint8), \n",
    "                         parsed_examples['img'], \n",
    "                         dtype = tf.uint8) # since the type of output of the tf.map_fn is different from it's input type, we necessarily have to specify the dtype of the output\n",
    "        lbls = tf.dtypes.cast(parsed_examples['lbl'], tf.uint8)\n",
    "        return imgs, lbls\n",
    "    \n",
    "    if type(example)== bytes:\n",
    "        parsed_example = tf.io.parse_single_example(example, feature_description)\n",
    "        img = tf.io.parse_tensor(parsed_example['img'].numpy(), out_type = tf.uint8)\n",
    "        lbl = tf.dtypes.cast(parsed_example['lbl'],  tf.uint8)\n",
    "    \n",
    "    elif str(type(example)) == \"<class 'tensorflow.core.example.example_pb2.Example'>\":\n",
    "        img = tf.io.parse_tensor(example.features.feature['img'].bytes_list.value[0], \n",
    "                                 out_type = tf.uint8)\n",
    "        lbl = tf.convert_to_tensor(example.features.feature['lbl'].int64_list.value[0],\n",
    "                                   dtype = tf.uint8)\n",
    "    else:\n",
    "        print('Error: The input is neither a serialized string nor an Example protobuf')\n",
    "        return\n",
    "    \n",
    "    return img, lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test to_Example_protobuf and from_Example_protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a sample record from train_imgs for testing purposes\n",
    "for item in train_imgs.take(1):\n",
    "    train_item = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Example protobuf\n",
      "image data was converted to Example and converted back faithfully: True\n",
      "label data was converted to Example and converted back faithfully: True\n",
      "Parsing serialized string\n",
      "image data was converted to Example and converted back faithfully: True\n",
      "label data was converted to Example and converted back faithfully: True\n"
     ]
    }
   ],
   "source": [
    "# convert the record to Example \n",
    "ex = to_Example_protobuf(train_item)\n",
    "\n",
    "# re-obtain the record from the Example protobuf and check its validity\n",
    "print('Parsing Example protobuf')\n",
    "img, lbl = from_Example_protobuf(ex)\n",
    "\n",
    "# check the validity of img, lbl\n",
    "print('image data was converted to Example and converted back faithfully: {}'.format(tf.math.reduce_all(img == train_item[0])))\n",
    "print('label data was converted to Example and converted back faithfully: {}'.format(tf.math.reduce_all(lbl == train_item[1])))\n",
    "\n",
    "print('Parsing serialized string')\n",
    "img, lbl = from_Example_protobuf(ex.SerializeToString())\n",
    "\n",
    "# check the validity of img, lbl\n",
    "print('image data was converted to Example and converted back faithfully: {}'.format(tf.math.reduce_all(img == train_item[0])))\n",
    "print('label data was converted to Example and converted back faithfully: {}'.format(tf.math.reduce_all(lbl == train_item[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data in TFRecord files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 50000 images in the training set. Let us divide them into 50 TFRecord files with each file containing serialized example protobufs for 1000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training set\n",
    "seed = 42\n",
    "buffer_size = 20000\n",
    "train_shuffled = train_imgs.shuffle(buffer_size = buffer_size, seed = seed)\n",
    "train_itr = iter(train_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file_num: 0\n",
      "****************************************************************************************************\n",
      "file_num: 1\n",
      "****************************************************************************************************\n",
      "file_num: 2\n",
      "****************************************************************************************************\n",
      "file_num: 3\n",
      "****************************************************************************************************\n",
      "file_num: 4\n",
      "****************************************************************************************************\n",
      "file_num: 5\n",
      "****************************************************************************************************\n",
      "file_num: 6\n",
      "****************************************************************************************************\n",
      "file_num: 7\n",
      "****************************************************************************************************\n",
      "file_num: 8\n",
      "****************************************************************************************************\n",
      "file_num: 9\n",
      "****************************************************************************************************\n",
      "file_num: 10\n",
      "****************************************************************************************************\n",
      "file_num: 11\n",
      "****************************************************************************************************\n",
      "file_num: 12\n",
      "****************************************************************************************************\n",
      "file_num: 13\n",
      "****************************************************************************************************\n",
      "file_num: 14\n",
      "****************************************************************************************************\n",
      "file_num: 15\n",
      "****************************************************************************************************\n",
      "file_num: 16\n",
      "****************************************************************************************************\n",
      "file_num: 17\n",
      "****************************************************************************************************\n",
      "file_num: 18\n",
      "****************************************************************************************************\n",
      "file_num: 19\n",
      "****************************************************************************************************\n",
      "file_num: 20\n",
      "****************************************************************************************************\n",
      "file_num: 21\n",
      "****************************************************************************************************\n",
      "file_num: 22\n",
      "****************************************************************************************************\n",
      "file_num: 23\n",
      "****************************************************************************************************\n",
      "file_num: 24\n",
      "****************************************************************************************************\n",
      "file_num: 25\n",
      "****************************************************************************************************\n",
      "file_num: 26\n",
      "****************************************************************************************************\n",
      "file_num: 27\n",
      "****************************************************************************************************\n",
      "file_num: 28\n",
      "****************************************************************************************************\n",
      "file_num: 29\n",
      "****************************************************************************************************\n",
      "file_num: 30\n",
      "****************************************************************************************************\n",
      "file_num: 31\n",
      "****************************************************************************************************\n",
      "file_num: 32\n",
      "****************************************************************************************************\n",
      "file_num: 33\n",
      "****************************************************************************************************\n",
      "file_num: 34\n",
      "****************************************************************************************************\n",
      "file_num: 35\n",
      "****************************************************************************************************\n",
      "file_num: 36\n",
      "****************************************************************************************************\n",
      "file_num: 37\n",
      "****************************************************************************************************\n",
      "file_num: 38\n",
      "****************************************************************************************************\n",
      "file_num: 39\n",
      "****************************************************************************************************\n",
      "file_num: 40\n",
      "****************************************************************************************************\n",
      "file_num: 41\n",
      "****************************************************************************************************\n",
      "file_num: 42\n",
      "****************************************************************************************************\n",
      "file_num: 43\n",
      "****************************************************************************************************\n",
      "file_num: 44\n",
      "****************************************************************************************************\n",
      "file_num: 45\n",
      "****************************************************************************************************\n",
      "file_num: 46\n",
      "****************************************************************************************************\n",
      "file_num: 47\n",
      "****************************************************************************************************\n",
      "file_num: 48\n",
      "****************************************************************************************************\n",
      "file_num: 49\n",
      "****************************************************************************************************"
     ]
    }
   ],
   "source": [
    "train_filepaths = []\n",
    "compression = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "\n",
    "for file_num in range(50):\n",
    "    filename = 'fashion_MNIST_TFRecord/train_tfrecord_'+ str(file_num)+'.tfrecord'\n",
    "    train_filepaths.append(filename)\n",
    "    print('\\nfile_num: {}'.format(file_num))\n",
    "    with tf.io.TFRecordWriter(filename, compression) as f:\n",
    "        for item_num in range(1000):\n",
    "            train_item = next(train_itr)\n",
    "            serialized_example = to_Example_protobuf(train_item)\n",
    "            f.write(serialized_example.SerializeToString())\n",
    "            if item_num%10 ==0:\n",
    "                print('*', end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10000 images in the validation set. We will store these in 10 TFRecord files, each containing serialized example protobufs for 1000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file_num: 0\n",
      "****************************************************************************************************\n",
      "file_num: 1\n",
      "****************************************************************************************************\n",
      "file_num: 2\n",
      "****************************************************************************************************\n",
      "file_num: 3\n",
      "****************************************************************************************************\n",
      "file_num: 4\n",
      "****************************************************************************************************\n",
      "file_num: 5\n",
      "****************************************************************************************************\n",
      "file_num: 6\n",
      "****************************************************************************************************\n",
      "file_num: 7\n",
      "****************************************************************************************************\n",
      "file_num: 8\n",
      "****************************************************************************************************\n",
      "file_num: 9\n",
      "****************************************************************************************************"
     ]
    }
   ],
   "source": [
    "val_filepaths = []\n",
    "val_itr = iter(val_imgs)\n",
    "\n",
    "for file_num in range(10):\n",
    "    filename = 'fashion_MNIST_TFRecord/val_tfrecord_'+ str(file_num) + '.tfrecord'\n",
    "    val_filepaths.append(filename)\n",
    "    print('\\nfile_num: {}'.format(file_num))\n",
    "    with tf.io.TFRecordWriter(filename, compression) as f:\n",
    "        for item_num in range(1000):\n",
    "            val_item = next(val_itr)\n",
    "            serialized_example = to_Example_protobuf(val_item)\n",
    "            f.write(serialized_example.SerializeToString())\n",
    "            if item_num%10 == 0:\n",
    "                print('*', end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set contains 10000 test images. We will create 10 TFRecord files for these, each storing the serialized example protobuf for 1000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file_num: 0\n",
      "****************************************************************************************************\n",
      "file_num: 1\n",
      "****************************************************************************************************\n",
      "file_num: 2\n",
      "****************************************************************************************************\n",
      "file_num: 3\n",
      "****************************************************************************************************\n",
      "file_num: 4\n",
      "****************************************************************************************************\n",
      "file_num: 5\n",
      "****************************************************************************************************\n",
      "file_num: 6\n",
      "****************************************************************************************************\n",
      "file_num: 7\n",
      "****************************************************************************************************\n",
      "file_num: 8\n",
      "****************************************************************************************************\n",
      "file_num: 9\n",
      "****************************************************************************************************"
     ]
    }
   ],
   "source": [
    "test_filepaths = []\n",
    "test_itr = iter(test_imgs)\n",
    "\n",
    "for file_num in range(10):\n",
    "    filename = 'fashion_MNIST_TFRecord/test_tfrecord_' + str(file_num) + '.tfrecord'\n",
    "    test_filepaths.append(filename)\n",
    "    print('\\nfile_num: {}'.format(file_num))\n",
    "    with tf.io.TFRecordWriter(filename, compression) as f:\n",
    "        for item_num in range(1000):\n",
    "            test_item = next(test_itr)\n",
    "            serialized_example = to_Example_protobuf(test_item)\n",
    "            f.write(serialized_example.SerializeToString())\n",
    "            if item_num%10==0:\n",
    "                print('*', end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset from the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generically, tf.data.TFRecordDataset accepts a list of filepaths\n",
    "# in order to specify files using glob patterns rather than a list of filepaths, we do the following\n",
    "# use tf.data.TFRecordDataset.list_files() to create a filepath_dataset corresponding to a file pattern\n",
    "# then pass this filepath_dataset to tf.data.TFRecordDataset\n",
    "train_filepaths = 'fashion_MNIST_TFRecord/train_tfrecord_*.tfrecord'\n",
    "filepath_dataset = tf.data.TFRecordDataset.list_files(train_filepaths)\n",
    "\n",
    "batch_size = 5000\n",
    "num_parallel_reads = 5\n",
    "train_data = tf.data.TFRecordDataset(filepath_dataset, \n",
    "                                     compression_type = 'GZIP',\n",
    "                                     num_parallel_reads = num_parallel_reads).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(train_data.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch, lbl_batch = from_Example_protobuf(item, batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5000, 28, 28), dtype=uint8, numpy=\n",
       "array([[[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   2,   0, ..., 174,  58,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,  30,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]]], dtype=uint8)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5000,), dtype=uint8, numpy=array([9, 8, 4, ..., 0, 2, 9], dtype=uint8)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5000, 28, 28])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
